{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78d270e-0241-4a4a-80d8-2a0dd38ffa54",
   "metadata": {},
   "source": [
    "# Rockfall monitoring\n",
    "\n",
    "Ronald Tabernig, William Albert, Hannah Weiser, July 2025 \n",
    "\n",
    "This notebook demonstrates the preparation of the rockfall monitoring example, including the change detection workflow using [py4dgeo](github.com/3dgeo-heidelberg/py4dgeo). The dataset contains **6 scans** of a [rockfall area in Trier, Germany](https://www.youtube.com/watch?v=bOp4XN9FM48) captured hourly in the night from 25 August 2024 to 26 August 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce57fc-d59a-441a-bbcf-1586238437f0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be0e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4dgeo\n",
    "import laspy\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, \"../src\")\n",
    "from fourdgeo import projection\n",
    "from fourdgeo import utilities\n",
    "from fourdgeo import change\n",
    "\n",
    "# File import and handling\n",
    "from pathlib import Path\n",
    "import zipfile, urllib.request, shutil\n",
    "from glob import glob\n",
    "\n",
    "# Image handling\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ff4c9",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba481409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle file download/reading here\n",
    "data_url = \"https://heibox.uni-heidelberg.de/f/6c2a4e6755b74d1abad0/?dl=1\"\n",
    "file_name = \"rockfall_trier.zip\"\n",
    "data_folder = \"data/rockfall_trier\"\n",
    "\n",
    "if not Path(data_folder).exists():\n",
    "    with urllib.request.urlopen(data_url) as response, open(file_name, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_folder)\n",
    "    \n",
    "    # Deleting the .zip file\n",
    "    os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e9120",
   "metadata": {},
   "source": [
    "## Change detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f822c-86b3-4feb-9324-17ede41cbdbb",
   "metadata": {},
   "source": [
    "Here, we perform change analysis between consecutive pairs of epochs, i.e., between the first and the second, the second and the third, and so on. We compute point cloud distances using the M3C2 algorithm ([Lague et al. 2013](https://doi.org/10.1016/j.isprsjprs.2013.04.009)) as [implemented in py4dgeo](https://py4dgeo.readthedocs.io/en/stable/m3c2.html), mask only significant changes, cluster the masked points using [DBSCAN](https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf) and finally extract change objects, which are defined by a polygon outline, attributes such as the mean M3C2 magnitude and the size, as well as a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962d8eb5-9a0b-4667-987f-41b9ddeb8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3c2_settings = {\"cyl_radius\":1,\n",
    "                 \"normal_radii\":[1.0,],\n",
    "                 \"max_distance\": 10.0,\n",
    "                 \"registration_error\":0.025\n",
    "                 }\n",
    "\n",
    "# DBScan parameters\n",
    "dbscan_eps = 1\n",
    "min_cluster_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ee7cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-11 11:14:21][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240825_220005.laz'\n",
      "[2025-07-11 11:14:22][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240825_230005.laz'\n",
      "[2025-07-11 11:14:22][INFO] Building KDTree structure with leaf parameter 10\n",
      "[2025-07-11 11:14:22][INFO] Building KDTree structure with leaf parameter 10\n",
      "No clusters found between 240825_220005.laz and 240825_230005.laz.\n",
      "[2025-07-11 11:14:33][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240825_230005.laz'\n",
      "[2025-07-11 11:14:33][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240826_000005.laz'\n",
      "[2025-07-11 11:14:33][INFO] Building KDTree structure with leaf parameter 10\n",
      "[2025-07-11 11:14:33][INFO] Building KDTree structure with leaf parameter 10\n",
      "No clusters found between 240825_230005.laz and 240826_000005.laz.\n",
      "[2025-07-11 11:14:43][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240826_000005.laz'\n",
      "[2025-07-11 11:14:43][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240826_010006.laz'\n",
      "[2025-07-11 11:14:43][INFO] Building KDTree structure with leaf parameter 10\n",
      "[2025-07-11 11:14:43][INFO] Building KDTree structure with leaf parameter 10\n",
      "[2025-07-11 11:14:54][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240826_010006.laz'\n",
      "[2025-07-11 11:14:54][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240826_020005.laz'\n",
      "[2025-07-11 11:14:55][INFO] Building KDTree structure with leaf parameter 10\n",
      "[2025-07-11 11:14:55][INFO] Building KDTree structure with leaf parameter 10\n",
      "No clusters found between 240826_010006.laz and 240826_020005.laz.\n",
      "[2025-07-11 11:15:04][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240826_020005.laz'\n",
      "[2025-07-11 11:15:04][INFO] Reading point cloud from file 'D:\\Software\\4dgeo\\docs\\data\\rockfall_trier\\240826_030005.laz'\n",
      "[2025-07-11 11:15:04][INFO] Building KDTree structure with leaf parameter 10\n",
      "[2025-07-11 11:15:04][INFO] Building KDTree structure with leaf parameter 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No clusters found between 240826_020005.laz and 240826_030005.laz.\n"
     ]
    }
   ],
   "source": [
    "observations = {\"observations\": []}\n",
    "\n",
    "# Gather & sort only the .laz files\n",
    "laz_paths = list(Path(data_folder).glob(\"*.laz\"))\n",
    "laz_paths = sorted(laz_paths)\n",
    "\n",
    "# Walk through each consecutive pair\n",
    "for prev_path, curr_path in zip(laz_paths, laz_paths[1:]):\n",
    "    prev_fname = os.path.basename(prev_path)\n",
    "    curr_fname = os.path.basename(curr_path)\n",
    "\n",
    "    startDateTime = utilities.iso_timestamp(prev_fname)\n",
    "    endDateTime   = utilities.iso_timestamp(curr_fname)\n",
    "\n",
    "    # Load point clouds\n",
    "    epoch_0 = py4dgeo.read_from_las(prev_path.resolve())  # make absolute, so py4dgeo does not download and check if the file is in py4dgeo-test-data\n",
    "    epoch_1 = py4dgeo.read_from_las(curr_path.resolve())\n",
    "\n",
    "    # Compute M3C2\n",
    "    m3c2 = py4dgeo.M3C2(\n",
    "        epochs=(epoch_0, epoch_1),\n",
    "        corepoints=epoch_0.cloud,\n",
    "        cyl_radius=m3c2_settings[\"cyl_radius\"],\n",
    "        normal_radii=m3c2_settings[\"normal_radii\"],\n",
    "        max_distance=m3c2_settings[\"max_distance\"],\n",
    "        registration_error=m3c2_settings[\"registration_error\"],\n",
    "    )\n",
    "    distances, uncertainties = m3c2.run()\n",
    "\n",
    "    # Mask & stack only significant changes\n",
    "    mask = np.abs(distances) >= uncertainties[\"lodetection\"]\n",
    "    if not mask.any():\n",
    "        print(f\"No significant changes between {prev_fname} â†’ {curr_fname}\")\n",
    "        continue\n",
    "\n",
    "    significant_pts = epoch_0.cloud[mask]\n",
    "    significant_d  = distances[mask]\n",
    "    changes = np.column_stack((significant_pts, significant_d))\n",
    "\n",
    "    # Cluster & extract geoObjects\n",
    "    labeled = change.cluster_m3c2_changes(changes, dbscan_eps, min_cluster_size)\n",
    "    geoObjects = change.extract_geoObjects_from_clusters(labeled, endDateTime, prev_fname, curr_fname)\n",
    "\n",
    "    observations[\"observations\"].append({\n",
    "        \"backgroundImageData\": {},\n",
    "        \"startDateTime\": startDateTime,\n",
    "        \"endDateTime\": endDateTime,\n",
    "        \"geoObjects\": geoObjects,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e4422-66cb-44b5-a2c9-597eeb7eaca0",
   "metadata": {},
   "source": [
    "Let's have a look at what a change `geoObject` contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890701f8-faab-4853-9bb8-606527f80c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '28e9510626384a709338ef6a0afc8bad',\n",
       " 'type': 'unknown',\n",
       " 'dateTime': '2024-08-26T01:00:06',\n",
       " 'geometry': {'type': 'Polygon',\n",
       "  'coordinates': [[-258.019, 158.27100000000002, 5.363],\n",
       "   [-258.015, 158.19400000000002, 5.831],\n",
       "   [-258.005, 158.187, 5.751],\n",
       "   [-258.005, 158.18800000000002, 5.672],\n",
       "   [-258.397, 158.428, 5.36],\n",
       "   [-258.177, 158.293, 5.04],\n",
       "   [-258.129, 158.151, 5.9510000000000005],\n",
       "   [-258.677, 158.487, 5.244],\n",
       "   [-258.25100000000003, 158.225, 5.156],\n",
       "   [-258.298, 158.255, 5.0],\n",
       "   [-258.615, 158.356, 6.099],\n",
       "   [-258.32, 158.17600000000002, 6.009],\n",
       "   [-259.543, 158.731, 5.434],\n",
       "   [-259.471, 158.687, 5.272],\n",
       "   [-259.38, 158.52700000000002, 6.253],\n",
       "   [-259.473, 158.584, 6.098],\n",
       "   [-259.046, 158.323, 4.811],\n",
       "   [-258.974, 158.214, 4.847],\n",
       "   [-260.13100000000003, 158.64000000000001, 5.308],\n",
       "   [-259.866, 158.478, 5.15],\n",
       "   [-260.162, 158.534, 6.9750000000000005],\n",
       "   [-260.031, 158.454, 6.892],\n",
       "   [-259.777, 158.3, 5.12],\n",
       "   [-260.225, 158.491, 7.009],\n",
       "   [-260.595, 158.431, 6.273],\n",
       "   [-260.55400000000003, 158.406, 6.111],\n",
       "   [-260.559, 158.409, 6.027],\n",
       "   [-260.64300000000003, 158.46, 5.547],\n",
       "   [-261.731, 158.92000000000002, 6.343],\n",
       "   [-261.337, 158.681, 5.849],\n",
       "   [-261.803, 158.873, 6.354],\n",
       "   [-261.598, 158.673, 7.025],\n",
       "   [-261.765, 158.775, 6.625],\n",
       "   [-261.791, 158.793, 6.465],\n",
       "   [-261.66700000000003, 158.607, 6.676]]},\n",
       " 'customEntityData': {'X_centroid': -259.8313391304348,\n",
       "  'Y_centroid': 158.48924782608697,\n",
       "  'Z_centroid': 5.991530434782609,\n",
       "  'm3c2_magnitude_abs_average_per_cluster': 1.0733661294671384,\n",
       "  'volume': 1.2427361073333316,\n",
       "  'surface_area': 11.436782819482161,\n",
       "  'surface_to_volume_ratio': 9.20290538916042,\n",
       "  'cluster_size_points': 230}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations[\"observations\"][2][\"geoObjects\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b37c0",
   "metadata": {},
   "source": [
    "## Projections\n",
    "### Prepare the configuration file\n",
    "\n",
    "We use a configuration dictionary that contains general project settings like the `output_folder` and the relevant settings for the point cloud projection. For the projection, parameters like the `camera_position` and the `resolution_cm` are essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31918c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    \"project_setting\": {\n",
    "        \"project_name\": \"Rockfall_monitoring\",\n",
    "        \"output_folder\": \"./out/rockfall_monitoring\",\n",
    "        \"temporal_format\": \"%y%m%d_%H%M%S\",\n",
    "        \"silent_mode\": True,\n",
    "        \"include_timestamp\": False\n",
    "    },\n",
    "    \"pc_projection\": {\n",
    "        \"pc_path\": \"\",\n",
    "        \"make_range_image\": True,\n",
    "        \"make_color_image\": False,\n",
    "        \"top_view\": False,\n",
    "        \"save_rot_pc\": False,\n",
    "        \"resolution_cm\": 12.5,\n",
    "        \"camera_position\": [\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0\n",
    "        ],\n",
    "        \"rgb_light_intensity\": 100,\n",
    "        \"range_light_intensity\": 10,\n",
    "        \"epsg\": None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea860316",
   "metadata": {},
   "source": [
    "### Generate the background images\n",
    "\n",
    "We now generate the background images. For this, we are using classes and functions from the `fourdgeo` library. The class `PCloudProjection` directly takes our configuration file as input and writes them into our specified `output_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf1dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "list_background_projections = []\n",
    "pcs = sorted(laz_paths)\n",
    "\n",
    "for enum, pc in enumerate(pcs):\n",
    "    lf = laspy.read(pc)\n",
    "    configuration['pc_projection']['pc_path'] = pc\n",
    "    project_name = configuration['project_setting']['project_name']\n",
    "    output_folder = configuration['project_setting']['output_folder']\n",
    "\n",
    "    background_projection = projection.PCloudProjection(\n",
    "        configuration = configuration,\n",
    "        project_name = project_name,\n",
    "        projected_image_folder = output_folder,\n",
    "    )\n",
    "    # First projection\n",
    "    if enum == 0:\n",
    "        (\n",
    "            ref_h_fov, ref_v_fov, ref_anchor_point_xyz, \n",
    "            ref_h_img_res, ref_v_img_res\n",
    "        ) = background_projection.project_pc(buffer_m = 0.5)\n",
    "    # Next projections using reference data\n",
    "    else:\n",
    "        background_projection.project_pc(\n",
    "            ref_theta=ref_h_fov[0],\n",
    "            ref_phi=ref_v_fov[0],\n",
    "            ref_anchor_point_xyz=None,\n",
    "            ref_h_fov=ref_h_fov,\n",
    "            ref_v_fov=ref_v_fov,\n",
    "            ref_h_img_res=ref_h_img_res,\n",
    "            ref_v_img_res=ref_v_img_res\n",
    "        )\n",
    "\n",
    "    bg_img = background_projection.bg_image_filename[0]\n",
    "    if bg_img[0] == \".\":\n",
    "        bg_img = bg_img[2:]\n",
    "\n",
    "    outfile = bg_img.split('.')[0] + f\"_{enum}_{enum+1}.\" + bg_img.split('.')[1]\n",
    "    to_replace = glob(f\"{output_folder}/{project_name}_*Image.tif\")[0]\n",
    "\n",
    "    try:\n",
    "        os.rename(bg_img, outfile)\n",
    "    except FileExistsError:\n",
    "        os.remove(outfile)\n",
    "        os.rename(bg_img, outfile)\n",
    "\n",
    "    images.append(outfile)\n",
    "\n",
    "    background_projection.bg_image_filename[0] = outfile\n",
    "    list_background_projections.append(background_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d14dca",
   "metadata": {},
   "source": [
    "### Project the change events onto the image background\n",
    "\n",
    "Here, we also project the change events onto the background image using the `ProjectChange` class. The `observation` GeoJSON files are written to the `output_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5cb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = configuration[\"project_setting\"][\"project_name\"]\n",
    "list_observation_projection = []\n",
    "\n",
    "for epoch_id, observation in enumerate(observations['observations']):\n",
    "    if observation['geoObjects'] is None:\n",
    "        list_observation_projection.append(None)\n",
    "        continue\n",
    "    background_projection = list_background_projections[epoch_id]\n",
    "\n",
    "    observation_projection = projection.ProjectChange(observation=observation,\n",
    "                            project_name=f\"{project_name}_{epoch_id}_{epoch_id+1}\",\n",
    "                            projected_image_path=background_projection.bg_image_filename[0],\n",
    "                            projected_events_folder=output_folder,\n",
    "                            epsg=None)\n",
    "\n",
    "    observation_projection.project_change()\n",
    "    list_observation_projection.append(observation_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef1e149",
   "metadata": {},
   "source": [
    "### Display the rockfall event in the study site, as soon as detected\n",
    "\n",
    "We generate a GIF of the time series and display the rockfall event using the polygon outlines as soon as it is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2e3ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "gif_path = \"../docs/img/rockfall_projections_plus_observations.gif\"\n",
    "font = ImageFont.load_default(size = 30)\n",
    "\n",
    "for enum, img in enumerate(images[1:]):\n",
    "    frm = Image.open(img).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(frm)\n",
    "    draw.text((500, 36), f\"Epoch: {enum}\", fill=(255, 255, 255), font=font)\n",
    "\n",
    "    observation_projection = list_observation_projection[enum]\n",
    "\n",
    "    # Load geojson\n",
    "    if observation_projection is not None:\n",
    "        with open(observation_projection.geojson_name, 'r') as f:\n",
    "            geojson_data = json.load(f)\n",
    "\n",
    "        for feature in geojson_data[\"features\"]:\n",
    "            coords = np.array(feature[\"geometry\"]['coordinates'][0])\n",
    "            coords[:,1] *= -1\n",
    "            coords = coords.reshape(len(coords)*2)\n",
    "            draw.polygon(list(coords), outline='yellow', width=4)\n",
    "\n",
    "    frames.append(frm)\n",
    "    \n",
    "\n",
    "frames[0].save(\n",
    "    gif_path,\n",
    "    save_all=True,\n",
    "    append_images=frames[1:],\n",
    "    duration=1000,\n",
    "    loop=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c154e60-e9e3-4fd2-9b4c-87654bf75e3a",
   "metadata": {},
   "source": [
    "<img src=\"img/rockfall_projections_plus_observations.gif\" style=\"width:900px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8699cee",
   "metadata": {},
   "source": [
    "## Extract the final JSON \n",
    "\n",
    "In order to include this data into the dashboard, we now need to convert the created geojson data to the dashboard data model. For this, we iterate through all geojson files and create their observation objects and the aggregate them into one final data model object and store it in the `output_folder`. This files can then be loaded with the 4DGeo Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bbefae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = utilities.DataModel([])\n",
    "\n",
    "for (i, observation_projection) in enumerate(list_observation_projection):\n",
    "    if observation_projection is None:\n",
    "        continue\n",
    "    \n",
    "    with open(observation_projection.geojson_name, 'r') as f:\n",
    "        geojson_data = json.load(f)\n",
    "    \n",
    "    bg_img = list_background_projections[i].bg_image_filename[0]\n",
    "    new_observations = utilities.convert_geojson_to_datamodel(\n",
    "        geojson=geojson_data,\n",
    "        bg_img=bg_img,\n",
    "        width=Image.open(bg_img).convert(\"RGB\").size[0],\n",
    "        height=Image.open(bg_img).convert(\"RGB\").size[1]\n",
    "    )\n",
    "\n",
    "    aggregated_data.observations.append(new_observations.observations)\n",
    "\n",
    "with open(f\"{output_folder}/final_data_model.json\", \"w\") as f:\n",
    "    f.write(aggregated_data.toJSON())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4Dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
