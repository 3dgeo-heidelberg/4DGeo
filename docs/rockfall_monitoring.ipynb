{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338756d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4dgeo\n",
    "from sklearn import cluster\n",
    "from scipy import spatial\n",
    "import os\n",
    "import numpy as np\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "def cluster_m3c2_changes(significant_changes, dbscan_eps, min_cluster_size):\n",
    "    \"\"\"\n",
    "    Cluster M3C2 changes using DBSCAN and return clusters with their properties.\n",
    "    :param significant_changes: Array of significant changes with shape (n, 4) where n is the number of points.\n",
    "    :param dbscan_eps: The maximum distance between two samples for one to be considered as\n",
    "    :param min_cluster_size: The minimum number of samples in a cluster.\n",
    "    :return: A list of clusters with their properties.\n",
    "    \"\"\"\n",
    "    # DBSCAN clustering\n",
    "    dbscan = cluster.DBSCAN(eps=dbscan_eps, min_samples=min_cluster_size)\n",
    "    labels = dbscan.fit_predict(significant_changes[:, :-1])\n",
    "\n",
    "    # Combine results and check that the labels are unique\n",
    "    all_changes_with_labels = np.column_stack((significant_changes, labels))\n",
    "\n",
    "    # Remove noise points (label -1)\n",
    "    all_changes_with_labels = all_changes_with_labels[all_changes_with_labels[:, -1] != -1]\n",
    "    return all_changes_with_labels\n",
    "\n",
    "def extract_geoObjects_from_clusters(all_changes_with_labels, endDateTime_, filename_0, filename_1):\n",
    "    \"\"\"\n",
    "    Extract observations from clusters of M3C2 changes.\n",
    "    :param all_changes_with_labels: Array of significant changes with labels.\n",
    "    :param endDateTime_: The end date and time of the observation.\n",
    "    :param filename_0: The filename of the first epoch.\n",
    "    :param filename_1: The filename of the second epoch.\n",
    "    :return: A list of observations with geo objects.\n",
    "    \"\"\"\n",
    "    # Extract unique cluster IDs and their counts\n",
    "    cluster_ids, cluster_count = np.unique(all_changes_with_labels[:, -1],return_counts=True)\n",
    "\n",
    "    # If no clusters found, continue to the next file\n",
    "    if len(cluster_ids) == 0:\n",
    "        print(f\"No clusters found between {filename_0} and {filename_1}.\")\n",
    "        return\n",
    "\n",
    "    # backgroundImageData_ = XXX\n",
    "\n",
    "    geoObjects_ = []\n",
    "\n",
    "    # If clusters found, print the largest and smallest cluster sizes    \n",
    "    for cluster_id in range(len(cluster_ids)):\n",
    "        xyz = all_changes_with_labels[all_changes_with_labels[:, -1] == cluster_ids[cluster_id], :3]\n",
    "        m3c2_distances = all_changes_with_labels[all_changes_with_labels[:, -1] == cluster_ids[cluster_id], -2]\n",
    "        convex_hull = spatial.ConvexHull(xyz)\n",
    "        volume = convex_hull.volume\n",
    "        area = convex_hull.area\n",
    "        surface_to_volume_ratio = area / volume if volume > 0 else float('inf')\n",
    "        m3c2_mean_distance = np.mean(np.abs(m3c2_distances))\n",
    "        vertices_of_hull = convex_hull.points[convex_hull.vertices]\n",
    "\n",
    "        # Create an geo object based on the cluster for the observations\n",
    "        dateTime_ = endDateTime_\n",
    "        # Create a unique ID for the cluster\n",
    "        id_ = uuid.uuid4().hex\n",
    "        # Type is \"unknown\", as we don't apply any classifier here\n",
    "        type_ = \"unknown\"\n",
    "\n",
    "        customEntityData_ = {\n",
    "            \"X_centroid\": np.mean(xyz[:, 0]),\n",
    "            \"Y_centroid\": np.mean(xyz[:, 1]),\n",
    "            \"Z_centroid\": np.mean(xyz[:, 2]),\n",
    "            \"m3c2_magnitude_abs_average_per_cluster\": m3c2_mean_distance,\n",
    "            \"volume\": volume,\n",
    "            \"surface_area\": area,\n",
    "            \"surface_to_volume_ratio\": surface_to_volume_ratio,\n",
    "            \"cluster_size_points\": int(cluster_count[cluster_id]),\n",
    "            }\n",
    "        \n",
    "        geometry_ = {\n",
    "            \"type\": \"Polygon\",\n",
    "            # \"coordinates\": XXXX # Use projected coordinates for the polygon\n",
    "            \"coordinates\": [vertices_of_hull[:, :2].tolist()]  # Only use X and Y coordinates for the polygon\n",
    "        }\n",
    "\n",
    "        geoObjects_.append({\n",
    "            \"id\": id_,\n",
    "            \"type\": type_,\n",
    "            \"dateTime\": dateTime_,\n",
    "            \"geometry\": geometry_,\n",
    "            \"customEntityData\": customEntityData_\n",
    "        })\n",
    "        return geoObjects_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba481409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle file download/reading here\n",
    "infolder = r\"D:\\daten\\projects\\aimon\\low_res_bad_attr_clipped\"\n",
    "\n",
    "m3c2_settings = {\"cyl_radius\":1,\n",
    "                 \"normal_radii\":[1.0,],\n",
    "                 \"max_distance\": 10.0,\n",
    "                 \"registration_error\":0.025\n",
    "                 }\n",
    "\n",
    "#DBScan parameters\n",
    "dbscan_eps = 1\n",
    "min_cluster_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ee7cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 14:06:31][INFO] Reading point cloud from file 'D:\\daten\\projects\\aimon\\low_res_bad_attr_clipped\\240825_220005.laz'\n",
      "[2025-06-30 14:06:31][INFO] Reading point cloud from file 'D:\\daten\\projects\\aimon\\low_res_bad_attr_clipped\\240825_230005.laz'\n",
      "[2025-06-30 14:06:31][INFO] Building KDTree structure with leaf parameter 10\n",
      "[2025-06-30 14:06:31][INFO] Building KDTree structure with leaf parameter 10\n",
      "No clusters found between 240825_220005.laz and 240825_230005.laz.\n",
      "[2025-06-30 14:06:34][INFO] Reading point cloud from file 'D:\\daten\\projects\\aimon\\low_res_bad_attr_clipped\\240826_000005.laz'\n",
      "[2025-06-30 14:06:34][INFO] Building KDTree structure with leaf parameter 10\n",
      "No clusters found between 240825_220005.laz and 240826_000005.laz.\n",
      "[2025-06-30 14:06:37][INFO] Reading point cloud from file 'D:\\daten\\projects\\aimon\\low_res_bad_attr_clipped\\240826_010006.laz'\n",
      "[2025-06-30 14:06:37][INFO] Building KDTree structure with leaf parameter 10\n",
      "[2025-06-30 14:06:41][INFO] Reading point cloud from file 'D:\\daten\\projects\\aimon\\low_res_bad_attr_clipped\\240826_020005.laz'\n",
      "[2025-06-30 14:06:41][INFO] Building KDTree structure with leaf parameter 10\n",
      "No clusters found between 240825_220005.laz and 240826_020005.laz.\n",
      "[2025-06-30 14:06:44][INFO] Reading point cloud from file 'D:\\daten\\projects\\aimon\\low_res_bad_attr_clipped\\240826_030005.laz'\n",
      "[2025-06-30 14:06:44][INFO] Building KDTree structure with leaf parameter 10\n",
      "No clusters found between 240825_220005.laz and 240826_030005.laz.\n"
     ]
    }
   ],
   "source": [
    "observations = {\"observations\": []}\n",
    "epoch_0 = None\n",
    "# Iterate over all files in the folder and compute M3C2 between subsequent files\n",
    "for filename_1 in os.listdir(infolder):\n",
    "    if not filename_1.endswith(\".laz\"):\n",
    "        continue\n",
    "    filepath_1 = os.path.join(infolder, filename_1)\n",
    "\n",
    "    timestamp = filename_1.split(\".\")[0]  # Extract the timestamp from the filename, assuming the format is like \"230101_120000.laz\"\n",
    "\n",
    "    # t as \"String in ISO 8601 format\"\n",
    "    endDateTime_ = datetime.datetime.strptime(timestamp, \"%y%m%d_%H%M%S\").isoformat()\n",
    "\n",
    "    # Read the point cloud\n",
    "    epoch_1 = py4dgeo.read_from_las(filepath_1)\n",
    "    \n",
    "    # If this is the first file, continue and set it as the previous epoch for the next iteration\n",
    "    if epoch_0 is None:\n",
    "        epoch_0 = epoch_1\n",
    "        startDateTime = endDateTime_\n",
    "        filename_0 = filename_1\n",
    "        filepath_0 = filepath_1\n",
    "        continue\n",
    "    \n",
    "    # Compute M3C2 between the previous and current point epoch\n",
    "    m3c2 = py4dgeo.M3C2(\n",
    "        epochs=(epoch_0, epoch_1),\n",
    "        corepoints = epoch_0.cloud,\n",
    "        cyl_radius=m3c2_settings[\"cyl_radius\"],\n",
    "        normal_radii=m3c2_settings[\"normal_radii\"],\n",
    "        max_distance=m3c2_settings[\"max_distance\"],\n",
    "        registration_error = m3c2_settings[\"registration_error\"]\n",
    "        )\n",
    "    \n",
    "    # Run M3C2 computation\n",
    "    m3c2_distances, uncertainties = m3c2.run()\n",
    "    \n",
    "    # Extract significant changes\n",
    "    significant_m3c2_change_mask = np.abs(m3c2_distances) >= uncertainties[\"lodetection\"]\n",
    "    points_with_significant_change = epoch_0.cloud[significant_m3c2_change_mask]\n",
    "    significant_distances = m3c2_distances[significant_m3c2_change_mask]\n",
    "    \n",
    "    # Update previous point cloud\n",
    "    epoch_0 = epoch_1\n",
    "\n",
    "    # Make one array with points and their distances\n",
    "    significant_changes = np.column_stack((points_with_significant_change, significant_distances))\n",
    "\n",
    "    # If no significant changes, continue to the next file\n",
    "    if significant_changes.shape[0] == 0:\n",
    "        print(f\"No significant changes found between {filename_0} and {filename_1}.\")\n",
    "        continue\n",
    "\n",
    "    # Cluster the significant changes\n",
    "    all_changes_with_labels = cluster_m3c2_changes(significant_changes, dbscan_eps, min_cluster_size)\n",
    "\n",
    "    geoObjects_ = extract_geoObjects_from_clusters(all_changes_with_labels, endDateTime_, filename_0, filename_1)\n",
    "\n",
    "\n",
    "    # Add the observation to the list\n",
    "    observations[\"observations\"].append({\n",
    "        \"backgroundImageData\":{},\n",
    "        \"startDateTime\": startDateTime,\n",
    "        \"endDateTime\": endDateTime_,\n",
    "        \"geoObjects\": geoObjects_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cdf5f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observations': [{'backgroundImageData': {},\n",
       "   'startDateTime': '2024-08-25T22:00:05',\n",
       "   'endDateTime': '2024-08-25T23:00:05',\n",
       "   'geoObjects': None},\n",
       "  {'backgroundImageData': {},\n",
       "   'startDateTime': '2024-08-25T22:00:05',\n",
       "   'endDateTime': '2024-08-26T00:00:05',\n",
       "   'geoObjects': None},\n",
       "  {'backgroundImageData': {},\n",
       "   'startDateTime': '2024-08-25T22:00:05',\n",
       "   'endDateTime': '2024-08-26T01:00:06',\n",
       "   'geoObjects': [{'id': '09e9ace359ae4fe4a6b1236e9b2df26e',\n",
       "     'type': 'unknown',\n",
       "     'dateTime': '2024-08-26T01:00:06',\n",
       "     'geometry': {'type': 'Polygon',\n",
       "      'coordinates': [[[-258.019, 158.27100000000002],\n",
       "        [-258.015, 158.19400000000002],\n",
       "        [-258.005, 158.187],\n",
       "        [-258.005, 158.18800000000002],\n",
       "        [-258.397, 158.428],\n",
       "        [-258.177, 158.293],\n",
       "        [-258.129, 158.151],\n",
       "        [-258.677, 158.487],\n",
       "        [-258.25100000000003, 158.225],\n",
       "        [-258.298, 158.255],\n",
       "        [-258.615, 158.356],\n",
       "        [-258.32, 158.17600000000002],\n",
       "        [-259.543, 158.731],\n",
       "        [-259.471, 158.687],\n",
       "        [-259.38, 158.52700000000002],\n",
       "        [-259.473, 158.584],\n",
       "        [-259.046, 158.323],\n",
       "        [-258.974, 158.214],\n",
       "        [-260.13100000000003, 158.64000000000001],\n",
       "        [-259.866, 158.478],\n",
       "        [-260.162, 158.534],\n",
       "        [-260.031, 158.454],\n",
       "        [-259.777, 158.3],\n",
       "        [-260.225, 158.491],\n",
       "        [-260.595, 158.431],\n",
       "        [-260.55400000000003, 158.406],\n",
       "        [-260.559, 158.409],\n",
       "        [-260.64300000000003, 158.46],\n",
       "        [-261.731, 158.92000000000002],\n",
       "        [-261.337, 158.681],\n",
       "        [-261.803, 158.873],\n",
       "        [-261.598, 158.673],\n",
       "        [-261.765, 158.775],\n",
       "        [-261.791, 158.793],\n",
       "        [-261.66700000000003, 158.607]]]},\n",
       "     'customEntityData': {'X_centroid': np.float64(-259.8313391304348),\n",
       "      'Y_centroid': np.float64(158.48924782608697),\n",
       "      'Z_centroid': np.float64(5.991530434782609),\n",
       "      'm3c2_magnitude_abs_average_per_cluster': np.float64(1.0733770225733077),\n",
       "      'volume': 1.2427361073333316,\n",
       "      'surface_area': 11.436782819482161,\n",
       "      'surface_to_volume_ratio': 9.20290538916042,\n",
       "      'cluster_size_points': 230}}]},\n",
       "  {'backgroundImageData': {},\n",
       "   'startDateTime': '2024-08-25T22:00:05',\n",
       "   'endDateTime': '2024-08-26T02:00:05',\n",
       "   'geoObjects': None},\n",
       "  {'backgroundImageData': {},\n",
       "   'startDateTime': '2024-08-25T22:00:05',\n",
       "   'endDateTime': '2024-08-26T03:00:05',\n",
       "   'geoObjects': None}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
